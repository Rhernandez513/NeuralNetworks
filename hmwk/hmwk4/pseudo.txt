NN Backpropagation Pseudo 

0. Init weights randomly, weights are a [3,24] matrix called 'w', the first column are the biases of the first layer
1. Init eta and epsilon to some small (less than 1) value
2. Init epoch_max to some large (greater than 1000) value
3. Init output_bias to 1 (the single bias of the single node in the second layer)
4. Init the Mean Squared Error to positive infinity
5. For epochs=1...n
    5.1 for i=1...n
        5.1.1 Perform a forward pass
            5.1.1.1 Calculate the local fields of the first layer
                v = w[0] +  (w[1] * x[i])
            5.1.1.2 Apply the non-linear transformation phi to produce the output of the first layer
                y_1 = phi(v)
            5.1.1.3 Calculate the local fields for the second layer
                u = (W[2] * y_1) + output_bias
            5.1.1.4 Apply the summing function to the local fields of the second layer
                sum = 0 
                sum += u[j] for all j in u
            5.1.1.5 Apply the non-linear transformation output_phi to produce the output of the second layer (output of network)
                y = output_phi(sum)
        5.1.2 Calculate the Error of the network output for the ith input
                L2_Error =  d_i - y
        5.1.3 If the absolute value of the  Error is greater than epsilon
                if |L2_Error| > epsilon
            5.1.3.1 Increment error count
                    errors += 1
            5.1.3.2 Update the output bias
                output_bias += eta * L2_Error 
            5.1.3.3 Update the weights from the first layer to the second using the derivative of the output layer's phi, output_phi prime using the local fields as input to output_phi_prime
                    As output_phi() was f(x) = x , output_phi_prime is f(x) = 1
                W[2] += eta * L2_Error * output_phi_prime(u) * (-x[i])
            5.1.3.4 Calculate the error of the first layer weights
                L1_Error = d_i - y_1
            5.1.3.5 Update the weights of the first layer using the derivative of phi, phi_prime with the local fields of the first layer as input to phi_prime
                    As phi() was f(x) = tanh(x) , phi_prime is f(x) = (1 / (cosh(v) ** 2))
                W[1] += eta * L1_Error * phi_prime(v) * (-x_i)
            5.1.3.6 Update the biases that are input to the first layer using the L1 Error
                W[0] += eta * L1_Error
        5.1.4 Calculate the Mean Squared Error
                new_mse = (1/n) * sigma(i=1 to n)(d_i - f(x_i, w))) ^ 2
            5.1.4.1 if new_mse > mse
                5.1.4.1.1 Decrease eta
                    eta *= 0.9
        5.1.5 Update the Mean Squared Error
                mse = new_mse
    5.2 Increment the epoch
        epoch += 1
